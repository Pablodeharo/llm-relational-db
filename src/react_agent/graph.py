from pathlib import Path
from typing import Literal

from langchain_core.messages import AIMessage, ToolMessage
from langgraph.graph import StateGraph
from langgraph.runtime import Runtime

from react_agent.context import Context
from react_agent.state import InputState, State
from react_agent.model_manager import ModelManager
from react_agent.backends.base import GenerationConfig
from react_agent.tool_router import ToolRouter
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver


async def call_model(state: State, runtime: Runtime[Context]):
    """
    Calls the LLM model and generates a response.
    
    Args:
        state: Current state containing the message history
        runtime: Runtime object holding execution context (model name, prompts, etc.)
        
    Returns:
        Updated state with a new AIMessage generated by the model
    """
    model_name = runtime.context.reasoning_model
    
    # Load the model backend
    model_manager = ModelManager(
        config_path=Path(__file__).parent / "config" / "models.yaml"
    )
    backend = await model_manager.get_backend(model_name)
    
    # Build the prompt from the system prompt and conversation history
    system_prompt = runtime.context.system_prompt
    
    # Extract content from all messages
    conversation_parts = []
    for msg in state.messages:
        if hasattr(msg, "content") and msg.content:
            # Identify message type for formatting
            if isinstance(msg, AIMessage):
                conversation_parts.append(f"Assistant: {msg.content}")
            elif isinstance(msg, ToolMessage):
                conversation_parts.append(f"Tool Result: {msg.content}")
            else:  # HumanMessage
                conversation_parts.append(f"User: {msg.content}")
    
    # Build the full prompt
    prompt = f"{system_prompt}\n\n" + "\n\n".join(conversation_parts)
    
    # Generate model response
    response = await backend.generate(
        prompt=prompt,
        config=GenerationConfig(
            max_tokens=512,
            temperature=0.7,
            top_p=0.9,
            stop_sequences=["</s>", "User:", "Tool Result:"]
        )
    )
    
    # Create AIMessage with content and optional tool calls
    ai_message = AIMessage(
        content=response.content,
        tool_calls=response.tool_calls if response.tool_calls else []
    )
    
    return {
        "messages": state.messages + [ai_message]
    }


async def route_tool(state: State, runtime: Runtime[Context]):
    """
    Executes the tools requested by the model.
    
    Args:
        state: State containing messages (last message must be an AIMessage with tool_calls)
        runtime: Runtime object holding execution context
        
    Returns:
        Updated state with a ToolMessage for each executed tool
    """
    tool_router = ToolRouter(runtime)
    last_msg = state.messages[-1]
    
    # Ensure the last message is an AIMessage with tool calls
    if not isinstance(last_msg, AIMessage):
        print("route_tool called but last message is not AIMessage")
        return {"messages": state.messages}
    
    if not last_msg.tool_calls:
        print("route_tool called but no tool_calls found")
        return {"messages": state.messages}
    
    # Execute each tool call
    tool_messages = []
    
    for call in last_msg.tool_calls:
        try:
            print(f"Executing tool: {call.name}")
            
            # Execute the tool
            result = await tool_router.execute_tool(
                tool_name=call.name,
                **call.arguments  # ToolCall.arguments is a dict
            )
            
            # Create ToolMessage with the result
            tool_msg = ToolMessage(
                content=str(result),
                tool_call_id=call.id,
                name=call.name
            )
            tool_messages.append(tool_msg)
            
            print(f"✓ Tool '{call.name}' executed successfully")
            
        except Exception as e:
            print(f"Error executing tool '{call.name}': {e}")
            
            # Create ToolMessage with the error
            error_msg = ToolMessage(
                content=f"Error: {str(e)}",
                tool_call_id=call.id,
                name=call.name
            )
            tool_messages.append(error_msg)
    
    return {
        "messages": state.messages + tool_messages
    }


def route_model_output(state: State) -> Literal["__end__", "tools"]:
    """
    Decides the next step based on the model output.
    
    If the model requested tools → "tools"
    Otherwise → "__end__" (terminate)
    
    Args:
        state: Current state containing messages
        
    Returns:
        Name of the next node ("tools" or "__end__")
    """
    messages = state.messages
    
    if not messages:
        print("No messages in state, ending")
        return "__end__"
    
    last = messages[-1]
    
    # Only AIMessage instances can contain tool calls
    if not isinstance(last, AIMessage):
        print(f"ℹLast message is {type(last).__name__}, not AIMessage, ending")
        return "__end__"
    
    # Check whether tool calls are present
    has_tool_calls = (
        hasattr(last, "tool_calls") and
        last.tool_calls is not None and
        len(last.tool_calls) > 0
    )
    
    if has_tool_calls:
        print(f"Model called {len(last.tool_calls)} tool(s), routing to tools")
        return "tools"
    else:
        print("No tool calls, ending conversation")
        return "__end__"


builder = StateGraph(State, input_schema=InputState, context_schema=Context)

# Nodes
builder.add_node("call_model", call_model)
builder.add_node("tools", route_tool)

# Edges
builder.add_edge("__start__", "call_model")
builder.add_conditional_edges("call_model", route_model_output)
builder.add_edge("tools", "call_model")  # After tools execution, return to the model

# Compile graph
graph = builder.compile(name="DataExplorer Agent")
