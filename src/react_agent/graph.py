"""
graph.py

Defines a custom ReAct agent that combines:
- An LLM for reasoning and conversation (call_model)
- External tools, such as PostgreSQL (query_postgres)
- Cyclical execution flow between model and tools

Fully compatible with LangGraph Platform.
"""

import asyncio
import torch
from transformers import pipeline
from datetime import UTC, datetime
from typing import Dict, List, Literal, cast

from langchain_core.messages import AIMessage
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.runtime import Runtime

from react_agent.context import Context
from react_agent.state import InputState, State
from react_agent.tools import TOOLS
from react_agent.utils import load_chat_model
#from langchain.chat_models import ChatOpenAI
from react_agent.models import get_agent_model_async

async def call_model(
    state: State, runtime: Runtime[Context]
) -> Dict[str, List[AIMessage]]:
    """
    Calls the language model (LLM) using the Transformers pipeline.

    - Loads model async from Context.model
    - Binds external tools
    - Formats system prompt from Context.system_prompt
    - Processes response and handles tool calls

    Args:
        state (State): Current conversation state.
        runtime (Runtime[Context]): Execution context.

    Returns:
        dict: Contains the list of messages generated by the model.
    """

    # Load model and tokenizer asynchronously
    tokenizer, model = await get_agent_model_async(runtime.context.model)

    # Create a Transformers pipeline for text-generation
    # Use GPU if available (RTX 3050)
    device = 0 if torch.cuda.is_available() else -1
    text_gen = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=device,
        torch_dtype=torch.float16 if device >= 0 else torch.float32,
        max_new_tokens=512,
    )

    # Bind tools (LangGraph tools wrapper)
    llm_wrapper = lambda prompts: text_gen(prompts[0]["content"], return_full_text=False)[0]["generated_text"]

    # Prepare system prompt
    system_message = runtime.context.system_prompt.format(
        system_time=datetime.now(tz=UTC).isoformat()
    )

    # Combine system + conversation messages
    input_prompt = [system_message] + [m.content for m in state.messages]
    full_prompt = "\n".join(input_prompt)

    # Run pipeline in async thread to avoid blocking
    generated_text = await asyncio.to_thread(llm_wrapper, [{"content": full_prompt}])

    # Create AIMessage object
    response = AIMessage(
        id="llm_output",
        content=generated_text,
        tool_calls=[],  # Tool integration can be added here if needed
    )

    return {"messages": [response]}


# -----------------------------
# Graph construction
# -----------------------------
builder = StateGraph(State, input_schema=InputState, context_schema=Context)

# Main node: the LLM
builder.add_node(call_model)

# Tools node (PostgreSQL or other external tools)
builder.add_node("tools", ToolNode(TOOLS))

# Entry point of the graph
builder.add_edge("__start__", "call_model")


# -----------------------------
# Conditional routing function
# -----------------------------
def route_model_output(state: State) -> Literal["__end__", "tools"]:
    """
    Determines the next node based on the model output.

    - If the model does not request tools, ends the flow (__end__)
    - If the model wants to use tools, routes to the tools node
    """
    last_message = state.messages[-1]
    if not isinstance(last_message, AIMessage):
        raise ValueError(
            f"Expected AIMessage in output edges, but got {type(last_message).__name__}"
        )

    # No tool calls → finish
    if not last_message.tool_calls:
        return "__end__"

    # Tool calls present → execute tools node
    return "tools"


# -----------------------------
# Configure edges
# -----------------------------
# Conditional edge: decides whether to go to __end__ or tools after call_model
builder.add_conditional_edges(
    "call_model",
    route_model_output,
)

# Normal edge: after tools, return to call_model
builder.add_edge("tools", "call_model")


graph = builder.compile(name="ReAct Agent")
